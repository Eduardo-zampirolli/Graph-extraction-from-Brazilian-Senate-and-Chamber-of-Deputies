# NER for Parliamentary Texts

This repository contains scripts for Named Entity Recognition (NER) in Brazilian parliamentary texts, focusing on the identification of persons (PESSOA).

## Project Files

### Main Scripts

- **ner.py**: Main script that performs Named Entity Recognition on texts using a pre-trained model. Generates `.grouped_entities.json` files with grouped entities.

- **create_annotations.py**: Creates `.annotated.txt` files from original texts and `.grouped_entities.json` files, inserting `<PESSOA:name>` tags at correct positions.

- **extract_annotation.py**: Extracts manually annotated entities from ground truth files, generating JSON files with extracted entities.

- **create_manual.py**: Creates grouped JSON files from extracted manual annotations.

- **compare_outputs.py**: Compares automatic NER results with manual annotations (ground truth), generating metric reports such as precision, recall, and F1-score.

## Important Methods

### Entity Merging

The merging process (`_merge_adjacent_entities`) combines adjacent or overlapping entities that likely belong to the same name. For example:

- Fragmented entities like "Senator" and "João Silva" are merged into "Senator João Silva"
- Names separated by spaces or connectors like "de", "da", "do" are united
- Overlapping entities are combined to form a single mention

### Entity Grouping

The grouping process (`_group_similar_persons`) identifies different variations of the same name and groups them under a canonical name:

- Uses text similarity to identify variations of the same name (e.g., "João Silva", "J. Silva", "Silva")
- Prioritizes more complete names as group representatives
- Maintains all occurrence positions for each grouped entity
- Uses the fuzzywuzzy library to calculate similarity between strings

### Testing Method

The evaluation process (`compare_outputs.py`) compares NER results with manual annotations:

- **True Positives (TP)**: Entities correctly identified and grouped
- **Detected but Misgrouped (DM)**: Entities detected but incorrectly grouped
- **False Negatives (FN)**: Ground truth entities not detected
- **False Positives (FP)**: Incorrectly detected entities

Calculates precision, recall, and F1-score metrics in two modes:
- **Strict**: Only TPs are considered correct
- **Flexible**: Both TPs and DMs are considered correct detections

## How to Use

### Text Processing

1. **Entity Recognition**:
   ```
   python ner.py <input_directory> <output_directory>
   ```
   Processes all `.txt` files in the input directory and generates `.grouped_entities.json` files in the output directory.

2. **Creating Annotated Files**:
   ```
   python create_annotations.py --original-dir <text_dir> --json-dir <json_dir> --output-dir <output_dir>
   ```
   Creates `.annotated.txt` files from original texts and JSON files generated by NER.

3. **Manual Annotation Extraction**:
   ```
   python extract_annotation.py <ground_truth_file> <json_output_file> <reconstructed_text_file>
   ```
   Extracts entities from a manual ground truth file.

4. **Results Comparison**:
   ```
   python compare_outputs.py <directory> <base_filename>
   ```
   Compares NER results with manual annotations and generates a metrics report.

## Workflow

1. Run `ner.py` to process texts and generate `.grouped_entities.json` files
2. Use `create_annotations.py` to generate `.annotated.txt` files with tags
3. For evaluation, extract manual annotations with `extract_annotation.py`
4. Compare results using `compare_outputs.py`